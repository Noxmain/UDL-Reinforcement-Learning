{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "708b4901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import Tuple, Dict, List, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import deque\n",
    "from typing import List, Tuple, Dict\n",
    "import logging\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc6f3541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment import GameState, BulletChessEnv\n",
    "from agent_dqn import ChessQNetwork, BulletExperienceReplay, BulletChessDQNAgent\n",
    "from agent_policy_value import ChessPolicyValueNetwork, BulletChessAlphaZeroAgent, MCTSNode, AdaptiveMCTS\n",
    "from utils import get_time_pressure_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80bb82d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_self_play(agent, env, episodes, max_steps_per_episode, target_update_freq):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        for step in range(max_steps_per_episode):\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # Zeitdruck aus info extrahieren, falls vorhanden\n",
    "            time_pressure = info.get(\"time_pressure\", 1.0)\n",
    "\n",
    "            agent.store_experience(state, action, reward, next_state, done, time_pressure)\n",
    "            agent.train_step()\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        if episode % target_update_freq == 0:\n",
    "            agent.update_target_network()\n",
    "\n",
    "        print(f\"Episode {episode+1}: Total Reward = {total_reward:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "670aadc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = -0.999000049952666\n",
      "Episode 2: Total Reward = -0.9990011167526245\n",
      "Episode 3: Total Reward = -0.999000016673406\n",
      "Episode 4: Total Reward = -0.999000016673406\n",
      "Episode 5: Total Reward = -0.999\n",
      "Episode 6: Total Reward = -0.9990007501244546\n",
      "Episode 7: Total Reward = -0.9990000167965889\n",
      "Episode 8: Total Reward = -0.9990000166535378\n",
      "Episode 9: Total Reward = -0.999\n",
      "Episode 10: Total Reward = -0.9990009167869885\n",
      "Episode 11: Total Reward = -0.9990000166654587\n",
      "Episode 12: Total Reward = -0.9990000166614851\n",
      "Episode 13: Total Reward = -0.999000033680598\n",
      "Episode 14: Total Reward = -0.999\n",
      "Episode 15: Total Reward = -0.9990014333923658\n",
      "Episode 16: Total Reward = -0.9990000164151192\n",
      "Episode 17: Total Reward = -0.9990000166416169\n",
      "Episode 18: Total Reward = -0.9990009500702223\n",
      "Episode 19: Total Reward = -0.9990000166893005\n",
      "Episode 20: Total Reward = -0.999\n",
      "Episode 21: Total Reward = -0.9990000166535378\n",
      "Episode 22: Total Reward = -0.9990000167806943\n",
      "Episode 23: Total Reward = -0.9990006498853365\n",
      "Episode 24: Total Reward = -0.999\n",
      "Episode 25: Total Reward = -0.9990005162159602\n",
      "Episode 26: Total Reward = -0.9990000171025594\n",
      "Episode 27: Total Reward = -0.9990000166614851\n",
      "Episode 28: Total Reward = -0.9990000167051951\n",
      "Episode 29: Total Reward = -0.999\n",
      "Episode 30: Total Reward = -0.9990000166455905\n",
      "Episode 31: Total Reward = -0.9990005166530609\n",
      "Episode 32: Total Reward = -0.9990000166932742\n",
      "Episode 33: Total Reward = -0.9990004667202632\n",
      "Episode 34: Total Reward = -0.9990000166416169\n",
      "Episode 35: Total Reward = -0.9990000166217486\n",
      "Episode 36: Total Reward = -0.9990000168085098\n",
      "Episode 37: Total Reward = -0.9990005497972171\n",
      "Episode 38: Total Reward = -0.9990004166603088\n",
      "Episode 39: Total Reward = -0.9990000167250633\n",
      "Episode 40: Total Reward = -0.9990000166296958\n",
      "Episode 41: Total Reward = -0.9990003666480383\n",
      "Episode 42: Total Reward = -0.9990004666805268\n",
      "Episode 43: Total Reward = -0.9990003326376279\n",
      "Episode 44: Total Reward = -0.9990000167608261\n",
      "Episode 45: Total Reward = -0.9990000166932742\n",
      "Episode 46: Total Reward = -0.9990000175674756\n",
      "Episode 47: Total Reward = -0.999000400118033\n",
      "Episode 48: Total Reward = -0.9990000167330106\n",
      "Episode 49: Total Reward = -0.9990000166773796\n",
      "Episode 50: Total Reward = -0.999\n"
     ]
    }
   ],
   "source": [
    "# Initialize environment and agent\n",
    "env = BulletChessEnv()\n",
    "agent = BulletChessDQNAgent()\n",
    "\n",
    "num_episodes = 50\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    legal_actions = env.get_legal_actions()  \n",
    "    time_pressure_level = \"moderate\"\n",
    "\n",
    "\n",
    "    while not done:\n",
    "        action = agent.select_action(state, legal_actions,time_pressure_level)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.store_experience(state, action, reward, next_state, done, time_pressure_level)\n",
    "        agent.train_step()\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        agent.save_model(f\"checkpoint_{episode + 1}.pth\")\n",
    "\n",
    "agent.save( \"chess_dqn_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fc2eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = -1\n",
      "Episode 2: Total Reward = -1\n",
      "Episode 3: Total Reward = -1\n",
      "Episode 4: Total Reward = -1\n",
      "Episode 5: Total Reward = -1\n",
      "Episode 6: Total Reward = -1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     13\u001b[0m     time_remaining \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mgame_state\u001b[38;5;241m.\u001b[39mwhite_time \u001b[38;5;28;01mif\u001b[39;00m env\u001b[38;5;241m.\u001b[39mgame_state\u001b[38;5;241m.\u001b[39mboard\u001b[38;5;241m.\u001b[39mturn \u001b[38;5;28;01melse\u001b[39;00m env\u001b[38;5;241m.\u001b[39mgame_state\u001b[38;5;241m.\u001b[39mblack_time\n\u001b[1;32m---> 14\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_remaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     uci_action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39maction_index_to_uci[action]\n",
      "File \u001b[1;32mc:\\Users\\saski\\OneDrive\\Dokumente\\GitHub\\UDL-Reinforcement-Learning\\agent_policy_value.py:322\u001b[0m, in \u001b[0;36mBulletChessAlphaZeroAgent.select_action\u001b[1;34m(self, game_env, time_remaining, temperature, return_probs)\u001b[0m\n\u001b[0;32m    319\u001b[0m     num_simulations \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;66;03m# Get action probabilities from MCTS\u001b[39;00m\n\u001b[1;32m--> 322\u001b[0m action_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmcts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgame_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_simulations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_remaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;66;03m# Apply temperature\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m temperature \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\saski\\OneDrive\\Dokumente\\GitHub\\UDL-Reinforcement-Learning\\agent_policy_value.py:206\u001b[0m, in \u001b[0;36mAdaptiveMCTS.search\u001b[1;34m(self, game_env, num_simulations, time_budget)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;66;03m# Run simulations\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_simulations):\n\u001b[1;32m--> 206\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlegal_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgame_env\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Perform one MCTS simulation\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;66;03m# Return visit count distribution as action probabilities\u001b[39;00m\n\u001b[0;32m    209\u001b[0m action_probs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m4096\u001b[39m)  \u001b[38;5;66;03m# Initialize probability array for all possible moves\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = BulletChessEnv()\n",
    "agent = BulletChessAlphaZeroAgent()\n",
    "num_episodes = 50\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    legal_actions = env.get_legal_actions()  \n",
    "    time_pressure_level = \"moderate\"\n",
    "\n",
    "    while not done:\n",
    "        time_remaining = env.game_state.white_time if env.game_state.board.turn else env.game_state.black_time\n",
    "        action = agent.select_action(env, time_remaining)\n",
    "        value = None\n",
    "        uci_action = agent.action_index_to_uci[action]\n",
    "        next_state, reward, done, _ = env.step(uci_action)\n",
    "        agent.train_network(epochs=10, batch_size=32)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        agent.save_model(f\"checkpoint_{episode + 1}.pth\")\n",
    "\n",
    "agent.save(agent.model.state_dict(), \"chess_alpha_zero_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c8c735",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable numpy.int64 object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m time_remaining \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mgame_state\u001b[38;5;241m.\u001b[39mwhite_time \u001b[38;5;28;01mif\u001b[39;00m env\u001b[38;5;241m.\u001b[39mgame_state\u001b[38;5;241m.\u001b[39mboard\u001b[38;5;241m.\u001b[39mturn \u001b[38;5;28;01melse\u001b[39;00m env\u001b[38;5;241m.\u001b[39mgame_state\u001b[38;5;241m.\u001b[39mblack_time\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Run MCTS to get move and policy distribution\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m action_index, policy_probs \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mselect_action(env, time_remaining, return_probs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     16\u001b[0m uci_action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39maction_index_to_uci[action_index]\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Store training data (you'll need a function to convert env state to tensor)\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable numpy.int64 object"
     ]
    }
   ],
   "source": [
    "env = BulletChessEnv()\n",
    "agent = BulletChessAlphaZeroAgent()\n",
    "num_episodes = 50\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    episode_data = []  # store (state_tensor, MCTS_probs, current_player)\n",
    "\n",
    "    while not done:\n",
    "        time_remaining = env.game_state.white_time if env.game_state.board.turn else env.game_state.black_time\n",
    "\n",
    "        # Run MCTS to get move and policy distribution\n",
    "        action_index, policy_probs = agent.select_action(env, time_remaining, return_probs=True)\n",
    "        uci_action = agent.action_index_to_uci[action_index]\n",
    "\n",
    "        # Store training data (you'll need a function to convert env state to tensor)\n",
    "        board_tensor = agent.board_to_tensor(env.game_state.board)\n",
    "        episode_data.append((board_tensor, policy_probs, 1 if env.game_state.board.turn else -1))\n",
    "\n",
    "        next_state, reward, done, _ = env.step(uci_action)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "    # Assign outcome to all positions (+1 win, -1 loss, 0 draw)\n",
    "    for i in range(len(episode_data)):\n",
    "        board_tensor, policy_probs, current_player = episode_data[i]\n",
    "        final_value = reward * current_player  # from that player's perspective\n",
    "        agent.memory.append((board_tensor, policy_probs, final_value))\n",
    "\n",
    "    # Train the network (on a batch of experiences)\n",
    "    agent.train_network(epochs=5, batch_size=32)\n",
    "\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        agent.save_model(f\"checkpoint_{episode + 1}.pth\")\n",
    "\n",
    "# Save final model\n",
    "agent.save(agent.model.state_dict(), \"chess_alpha_zero_model.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-chess",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
