{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "708b4901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import Tuple, Dict, List, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import deque\n",
    "from typing import List, Tuple, Dict\n",
    "import logging\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc6f3541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment import GameState, BulletChessEnv\n",
    "from agent_dqn import ChessQNetwork, BulletExperienceReplay, BulletChessDQNAgent\n",
    "from agent_policy_value import ChessPolicyValueNetwork, BulletChessAlphaZeroAgent, MCTSNode, AdaptiveMCTS\n",
    "from utils import get_time_pressure_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80bb82d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_self_play(agent, env, episodes, max_steps_per_episode, target_update_freq):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        for step in range(max_steps_per_episode):\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # Zeitdruck aus info extrahieren, falls vorhanden\n",
    "            time_pressure = info.get(\"time_pressure\", 1.0)\n",
    "\n",
    "            agent.store_experience(state, action, reward, next_state, done, time_pressure)\n",
    "            agent.train_step()\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        if episode % target_update_freq == 0:\n",
    "            agent.update_target_network()\n",
    "\n",
    "        print(f\"Episode {episode+1}: Total Reward = {total_reward:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670aadc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment and agent\n",
    "env = BulletChessEnv()\n",
    "agent = BulletChessDQNAgent()\n",
    "\n",
    "num_episodes = 50\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    legal_actions = env.get_legal_actions()  \n",
    "    time_pressure_level = \"moderate\"\n",
    "\n",
    "\n",
    "    while not done:\n",
    "        action = agent.select_action(state, legal_actions,time_pressure_level)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.store_experience(state, action, reward, next_state, done, time_pressure_level)\n",
    "        agent.train_step()\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        agent.save_model(f\"checkpoint_{episode + 1}.pth\")\n",
    "\n",
    "torch.save(agent.q_network.state_dict(), \"chess_dqn_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fc2eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BulletChessEnv()\n",
    "agent = BulletChessAlphaZeroAgent()\n",
    "num_episodes = 50\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    legal_actions = env.get_legal_actions()  \n",
    "    time_pressure_level = \"moderate\"\n",
    "\n",
    "    while not done:\n",
    "        time_remaining = env.game_state.white_time if env.game_state.board.turn else env.game_state.black_time\n",
    "        action = agent.select_action(env, time_remaining)\n",
    "        value = None\n",
    "        uci_action = agent.action_index_to_uci[action]\n",
    "        next_state, reward, done, _ = env.step(uci_action)\n",
    "        agent.train_network(epochs=10, batch_size=32)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        agent.save_model(f\"checkpoint_{episode + 1}.pth\")\n",
    "\n",
    "torch.save(agent.model.state_dict(), \"chess_alpha_zero_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01c8c735",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable numpy.int64 object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m time_remaining = env.game_state.white_time \u001b[38;5;28;01mif\u001b[39;00m env.game_state.board.turn \u001b[38;5;28;01melse\u001b[39;00m env.game_state.black_time\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Run MCTS to get move and policy distribution\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m action_index, policy_probs = agent.select_action(env, time_remaining, return_probs=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     16\u001b[39m uci_action = agent.action_index_to_uci[action_index]\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Store training data (you'll need a function to convert env state to tensor)\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: cannot unpack non-iterable numpy.int64 object"
     ]
    }
   ],
   "source": [
    "env = BulletChessEnv()\n",
    "agent = BulletChessAlphaZeroAgent()\n",
    "num_episodes = 50\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    episode_data = []  # store (state_tensor, MCTS_probs, current_player)\n",
    "\n",
    "    while not done:\n",
    "        time_remaining = env.game_state.white_time if env.game_state.board.turn else env.game_state.black_time\n",
    "\n",
    "        # Run MCTS to get move and policy distribution\n",
    "        action_index, policy_probs = agent.select_action(env, time_remaining, return_probs=True)\n",
    "        uci_action = agent.action_index_to_uci[action_index]\n",
    "\n",
    "        # Store training data (you'll need a function to convert env state to tensor)\n",
    "        board_tensor = agent.board_to_tensor(env.game_state.board)\n",
    "        episode_data.append((board_tensor, policy_probs, 1 if env.game_state.board.turn else -1))\n",
    "\n",
    "        next_state, reward, done, _ = env.step(uci_action)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "    # Assign outcome to all positions (+1 win, -1 loss, 0 draw)\n",
    "    for i in range(len(episode_data)):\n",
    "        board_tensor, policy_probs, current_player = episode_data[i]\n",
    "        final_value = reward * current_player  # from that player's perspective\n",
    "        agent.memory.append((board_tensor, policy_probs, final_value))\n",
    "\n",
    "    # Train the network (on a batch of experiences)\n",
    "    agent.train_network(epochs=5, batch_size=32)\n",
    "\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        agent.save_model(f\"checkpoint_{episode + 1}.pth\")\n",
    "\n",
    "# Save final model\n",
    "torch.save(agent.model.state_dict(), \"chess_alpha_zero_model.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-chess",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
