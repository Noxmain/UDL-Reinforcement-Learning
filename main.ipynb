{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "708b4901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import Tuple, Dict, List, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import deque\n",
    "from typing import List, Tuple, Dict\n",
    "import logging\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc6f3541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment import GameState, BulletChessEnv\n",
    "from agent_dqn import ChessQNetwork, BulletExperienceReplay, BulletChessDQNAgent\n",
    "from agent_policy_value import ChessPolicyValueNetwork, BulletChessAlphaZeroAgent, MCTSNode, AdaptiveMCTS\n",
    "from utils import get_time_pressure_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80bb82d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_self_play(agent, env, episodes, max_steps_per_episode, target_update_freq):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        for step in range(max_steps_per_episode):\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # Zeitdruck aus info extrahieren, falls vorhanden\n",
    "            time_pressure = info.get(\"time_pressure\", 1.0)\n",
    "\n",
    "            agent.store_experience(state, action, reward, next_state, done, time_pressure)\n",
    "            agent.train_step()\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        if episode % target_update_freq == 0:\n",
    "            agent.update_target_network()\n",
    "\n",
    "        print(f\"Episode {episode+1}: Total Reward = {total_reward:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "670aadc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = -0.999\n",
      "Episode 2: Total Reward = -0.999\n",
      "Episode 3: Total Reward = -0.999\n",
      "Episode 4: Total Reward = -0.999\n",
      "Episode 5: Total Reward = -0.999\n",
      "Episode 6: Total Reward = -0.9990000334103902\n",
      "Episode 7: Total Reward = -0.999\n",
      "Episode 8: Total Reward = -0.999\n",
      "Episode 9: Total Reward = -0.999\n",
      "Episode 10: Total Reward = -0.999\n",
      "Episode 11: Total Reward = -0.999\n",
      "Episode 12: Total Reward = -0.999\n",
      "Episode 13: Total Reward = -0.999\n",
      "Episode 14: Total Reward = -0.999\n",
      "Episode 15: Total Reward = -0.999\n",
      "Episode 16: Total Reward = -0.999\n",
      "Episode 17: Total Reward = -0.9990000333825747\n",
      "Episode 18: Total Reward = -0.999\n",
      "Episode 19: Total Reward = -0.999\n",
      "Episode 20: Total Reward = -0.999\n",
      "Episode 21: Total Reward = -0.999\n",
      "Episode 22: Total Reward = -0.999\n",
      "Episode 23: Total Reward = -0.9990002008120219\n",
      "Episode 24: Total Reward = -0.999\n",
      "Episode 25: Total Reward = -0.9990000334938367\n",
      "Episode 26: Total Reward = -0.999\n",
      "Episode 27: Total Reward = -0.999\n",
      "Episode 28: Total Reward = -0.999\n",
      "Episode 29: Total Reward = -0.9990001675049464\n",
      "Episode 30: Total Reward = -0.999\n",
      "Episode 31: Total Reward = -0.999\n",
      "Episode 32: Total Reward = -0.999\n",
      "Episode 33: Total Reward = -0.999\n",
      "Episode 34: Total Reward = -0.9990001666943232\n",
      "Episode 35: Total Reward = -0.9990000336647034\n",
      "Episode 36: Total Reward = -0.999\n",
      "Episode 37: Total Reward = -0.999\n",
      "Episode 38: Total Reward = -0.999\n",
      "Episode 39: Total Reward = -0.999\n",
      "Episode 40: Total Reward = -0.999\n",
      "Episode 41: Total Reward = -0.999\n",
      "Episode 42: Total Reward = -0.999000167675813\n",
      "Episode 43: Total Reward = -0.999\n",
      "Episode 44: Total Reward = -0.9990001710335413\n",
      "Episode 45: Total Reward = -0.9990001833041509\n",
      "Episode 46: Total Reward = -0.999\n",
      "Episode 47: Total Reward = -0.999\n",
      "Episode 48: Total Reward = -0.999\n",
      "Episode 49: Total Reward = -0.999\n",
      "Episode 50: Total Reward = -0.999\n"
     ]
    }
   ],
   "source": [
    "# Initialize environment and agent\n",
    "env = BulletChessEnv()\n",
    "agent = BulletChessDQNAgent()\n",
    "\n",
    "num_episodes = 50\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    legal_actions = env.get_legal_actions()  \n",
    "    time_pressure_level = \"moderate\"\n",
    "\n",
    "\n",
    "    while not done:\n",
    "        action = agent.select_action(state, legal_actions,time_pressure_level)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.store_experience(state, action, reward, next_state, done, time_pressure_level)\n",
    "        agent.train_step()\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        agent.save_model(f\"checkpoint_{episode + 1}.pth\")\n",
    "\n",
    "torch.save(agent.q_network.state_dict(), \"chess_dqn_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fc2eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = -1\n",
      "Episode 2: Total Reward = -1\n",
      "Episode 3: Total Reward = -1\n",
      "Episode 4: Total Reward = -1\n",
      "Episode 5: Total Reward = -1\n",
      "Episode 6: Total Reward = -1\n",
      "Episode 7: Total Reward = -1\n"
     ]
    }
   ],
   "source": [
    "env = BulletChessEnv()\n",
    "agent = BulletChessAlphaZeroAgent()\n",
    "num_episodes = 50\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    legal_actions = env.get_legal_actions()  \n",
    "    time_pressure_level = \"moderate\"\n",
    "\n",
    "    while not done:\n",
    "        time_remaining = env.game_state.white_time if env.game_state.board.turn else env.game_state.black_time\n",
    "        action = agent.select_action(env, time_remaining)\n",
    "        value = None\n",
    "        uci_action = agent.action_index_to_uci[action]\n",
    "        next_state, reward, done, _ = env.step(uci_action)\n",
    "        agent.train_network(epochs=10, batch_size=32)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        agent.save_model(f\"checkpoint_{episode + 1}.pth\")\n",
    "\n",
    "torch.save(agent.model.state_dict(), \"chess_alpha_zero_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c8c735",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BulletChessEnv()\n",
    "agent = BulletChessAlphaZeroAgent()\n",
    "num_episodes = 50\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    episode_data = []  # store (state_tensor, MCTS_probs, current_player)\n",
    "\n",
    "    while not done:\n",
    "        time_remaining = env.game_state.white_time if env.game_state.board.turn else env.game_state.black_time\n",
    "\n",
    "        # Run MCTS to get move and policy distribution\n",
    "        action_index, policy_probs = agent.select_action(env, time_remaining, return_probs=True)\n",
    "        uci_action = agent.action_index_to_uci[action_index]\n",
    "\n",
    "        # Store training data (you'll need a function to convert env state to tensor)\n",
    "        board_tensor = agent.board_to_tensor(env.game_state.board)\n",
    "        episode_data.append((board_tensor, policy_probs, 1 if env.game_state.board.turn else -1))\n",
    "\n",
    "        next_state, reward, done, _ = env.step(uci_action)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "    # Assign outcome to all positions (+1 win, -1 loss, 0 draw)\n",
    "    for i in range(len(episode_data)):\n",
    "        board_tensor, policy_probs, current_player = episode_data[i]\n",
    "        final_value = reward * current_player  # from that player's perspective\n",
    "        agent.memory.append((board_tensor, policy_probs, final_value))\n",
    "\n",
    "    # Train the network (on a batch of experiences)\n",
    "    agent.train_network(epochs=5, batch_size=32)\n",
    "\n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        agent.save_model(f\"checkpoint_{episode + 1}.pth\")\n",
    "\n",
    "# Save final model\n",
    "torch.save(agent.model.state_dict(), \"chess_alpha_zero_model.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-chess",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
